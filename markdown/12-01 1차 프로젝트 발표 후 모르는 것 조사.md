TODO

오늘 발표 중 새로운 것 정리!

<br>

----

<br>



## class mode = sparse 

- ImageDataGenerator의 파라미터
  - 만들어진 디렉토리의 이름이 integer 이고 이름 그대로 사용 할 때 
    - ex 거리 디렉토리 1, 산 디렉토리 2, .... 1은 거리, 2는 산 
    - 같은 형식으로 y값을 만들어줌.
- 위와 같은 정수로 이루어진 라벨링이라면
  - loss='sparese_categorical_crossentropy'

<br>

------

<br>



## kernel_initializer 

히든레이어가 많아질 수록 가중치 폭발, 소실이 생길 수 있다.	

모델이 복잡하고 커질 수록 오랜 학습시간을 가진다.

모델이 복잡할 수록 오버피팅 위험이 크다.

- 출력층에서 입력층으로 되돌아가며 w를 업데이트 진행
  - 이 많은 노드와 레이어는 w를 업데이트하는 파라미터인 Gredient를 소실, 폭주시킴
  - 모든 가중치 초기화를 같은 값으로 진행하면 같은 값으로 w를 갱신 하므로 1개의 노드를 가지는 것과 같은 상황이 발생 
  - rel의 출력은 음수는 0이므로 고른 분포를위해 루트를 사용 하여 고른 출력분포를 제공한다. 'he_uniform', 'he_nomal'
  - 활성함수가 sigmoid라면 Xavier, relu라면 He
    - nomal은 정규분포, 가우시안분포
    - Uniform 균등 분포 / 특정 범위 내의 균당하게 나타나는 그래프
  - 깊은 CNN 신경망의 경우 정규분포의 초기화값을 사용하는 것이 좋은 결과를 나타내는 경우가 많다.
  - 경훈 씨는 Conv2D 에서 적용

## kernel_regularizer

각 레이어 출력에 대해 패널티를 적용하여 overfit을 방지하기 위한 전략

- Weight regularization(가중치 규제) :
  - L1: 모델이 예측하는 값과 라벨 실제 값 차이의 절대값에 기초한 손실) 함수입니다. L1 손실은 2 손실보다 이상점에 둔감합니다.
  - L2: 가중치 제곱의 합에 비례하여 가중치에 페널티를 주는 정규화 유형입니다. 정규화는 선형 모델의 일반화를 항상 개선합니다.



```
Dense(64, kernel_regularizer=regularizers.l2(0.001)) #l1은 출력의 절대값을 구하고 0.001을 곱한 결과를 출력,l2라면 절대값 대신 제곱
```

<br>

-----

<br>



## batch normalization vs Dropout

출력의 값을 정규분포로 만들어준다.

- 학습 속도 개선
- 가중치 초기값 선택의 의존성이 낮아짐(출력의 값을 정규화)  = kernel_initializer 유사
- 과적합의 위험을 줄임 (드롭아웃의 대채기법으로 사용가능)
- Gradient Vanishing 문제 해결



kernel이란?

<br>

---

<br>





## 완전제거법

결측 데이터(누락, 유효하지 못한 데이터)

- 완전제거법
  - 완전제거법은 결측치를 처리하는 방법중 하나로 누락된 컬럼이 존재하는 행을 제거하는 방법

- 합리적 접근법
  - 다른 컬럼의 인과관계를 통해 결측치를 채워넣는 방법





<br>

---

<br>



## label Encoding 

데이터 전처리에 사용하는 방법으로 문자로 그룹화 되어 있는 컬럼을 숫자로 라벨링 해준다.

```
import numpy as np
from sklearn.preprocessing import LabelEncoder

a = ['apple', 'samsung', 'LG']

a = np.array(a)
encoder = LabelEncoder()
result = encoder.fit_transform(a)
print(result)

#[1,2,0] 0부터 abcd 순차
```



<br>

---

<br>



## 부분 PCA

PCA 를 진행 시 모든 컬럼에 대해 진행 하는 것이 아닌 부분적인 컬럼에 대해 PCA를 적용 할 수있다.

방법은 잘 모르겠음 .... 물어보자!



<br>

------

<br>





## xgbosst gpu옵션

tree_method='gpu_hist', # gpu사용
predictor='gpu_predictor', ## gpu 사용

```
xgb = xgboost.XGBClassifier(
# feature_names=index,
tree_method='gpu_hist', # gpu사용
predictor='gpu_predictor', ## gpu 사용
reg_alpha=0.15, #default 1, 능선 회쉬(Ridge Regression)의 L2 정규화
reg_lamdba=0.85, #default 0, 라쏘 회귀(Lasso Regression)의 L1 정규화, 차원이 높은 경우 알고리즘 속도를 높임
# objective= 'binary:logistic',
# eval_metric= 'auc', 
random_state=SEED
)
```

<br>

-----

<br>





kernel이 뭐지?

