

## 딥러닝과 머신러닝의 차이



머신러닝은 딥러닝을 포함한 개념

- 머신러닝
  - 데이터로부터 패턴을 분석하여 예측하는 방법
  - 사람이 직접 특징을 추출한 정보를 기반으로 학습
  - 상대적으로 적은 데이터로 좋은 성능을 낼 수 있음
- 딥러닝
  - 분류를 통해 결과를 예측하는 것 
  - 컴퓨터가 스스로 중요한 특징을 추출하여 학습
  - 컴퓨터가 합당한 특징을 추출 할 수 있도록 상대적으로 많은 데이터가 필요



<br>

## 활성함수란?

입력받은 데이터를 적절하게 처리하여 출력하는 함수

- sigmoid

  - 항상 0과 1 **사이** 의 값만 출력하는 함수
  - Step 함수는 0 혹은 1 중간값이 없다. 
  - 이진분류 시 사용되는 함수

- Relu

  - 선형함수로  w <= 0 라면 0으로 w >= 0 w로 출력하는 함수
  - Gradient Vanishing을 해결하기 위해 많이 사용 되는 함수 
  - 유사한 함수로 Selu, Elu 등이 존재
    - 유사한 함수들은 0이하의 값들이 소실 되는 것을 방지하기위해 0이 아닌 0에 가까운 작은 값으로 반환

- SoftMax

  - 다중분류 문제를 풀 때 많이 사용되는 함수

  - 모든 출력의 합이 1이 되게 만들어 출력

    



<br>

## 손실함수란?(loss function, Cost Function)

입력을 받아 훈련된 가중치(w)와 바이어스(b)를 사용하여 기대출력을 만들어내고 실제 출력과 기대출력간의 차

이러한 솔실함수의 결과를 사용하여 W,B를 수정한다.

- MSE
  - 예측값과 실제 값 사이의 평균 제곱오차를 구함
  - 제곱연산으로 오차가 크다면 더욱 뚜렷하게 나타남
  - 제곱으로 인해 오차가 양수, 음수 모두 양수로 표현됨
- RMSE
  - 예측값과 실제값의 평균오차에 루트를 씌운것
  - MSE는 제곱 연산으로 인해 실제 오류보다 값이 커져 왜곡이 커짐
  - RMSE는 루트의 사용으로 왜곡이 줄어듬
- Binary Crossentropy
  - 실제 레이블과 예측 레이블간 교차 엔트로피 손실을 계산
  - 주로 이진 분류 시 사용
- Categorical Crossentropy
  - 실제 레이블과 예측 레이블간 교차 엔트로피 손실을 계산
  - 주로 다중분 시 사용



<br>



## 경사하강법이란?(Gradient-decent)

좁은 범위에서 기울기가 가장 큰 방향으로 그래프를 내려가는 방법으로 손실함수가 최소값을 찾아가는 방법

- Learning Rate (학습 조절 변수)
  - 경사 하강 시 사용되는 수치로 그래프에서 다음 위치를 지정 할 때 사용
  - 높은 LR은 곱연산으로 인해 값이 발산하게 됨
  - 낮은 LR은 손실함수가 0으로 가는 시간을 매우 느리게 함
  - 원하는 위치보다 높은 위치에서 멈출 수 있다 Local minima

<img height=400 src='https://raw.githubusercontent.com/angeloyeo/angeloyeo.github.io/master/pics/2020-08-16-gradient_descent/pic5.png'>

<br>

## Back-propagation

출력부터 반대 방향으로 순차적으로 활성함수의 결과 값을 편미분을 수행해 가면서 w와 b의 값을 갱신시킴

실질적으로 w와 b를 수정하는 것은 역전파(back propagation)

그래프의 이동거리는 **LR, 활성함수 결과의 편미분의 영향** 을 받는다.



<br>



## Gradient Vanishing이란?

w, b의 갱신에 사용되는 LR, 활성함수의 **편미분 값이 작아지게 되면 그래프에서의 움직임이 수축**된다. = 학습이 매우 오래걸리거나 로컬 미니마의 위험이 있다.

- Exploding 
  - LR이 너무  높게 되면 움직임이 매우 커지게 되어 발산한다.

<br>



## overffit과 underffit

공통점: 예측성능 저하

- Overffiting
  - 학습 데이터에 대해 과적합 되어있는 상태로 새로운 데이터에 대한 유연한 예측이 불가
  - 너무 많은 feature를 사용 할 시 발생
  - 많은 epoch(학습)을 진행할 시 발생
  - 편중된 학습데이터
  - Dropout사용
- Underffiting
  - 충분한 학습이 진행되지 않아 유효한 예측이 불가
  - 너무 적은 feature사용으로 유효한 특징, 패턴을 찾을 수 없는 상황
  - 충분치 못한 데이터



Todo 

Dropout이 무엇인지

Optimizer